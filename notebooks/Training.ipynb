{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3498f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install required libraries\n",
    "# RUN THIS CELL FIRST TO INSTALL ALL DEPENDENCIES\n",
    "\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install transformers\n",
    "!pip install ultralytics\n",
    "!pip install opencv-python\n",
    "!pip install pillow\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install kagglehub\n",
    "!pip install scikit-learn\n",
    "!pip install pyyaml\n",
    "!pip install pandas\n",
    "\n",
    "print(\"All required libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815bf48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import all libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c88606c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base models...\n",
      "YOLO model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RDHQ\\Downloads\\RADI-Image\\.radi_image_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load pre-trained models (BASE MODELS)\n",
    "print(\"Loading base models...\")\n",
    "yolo_model = YOLO('yolov8n.pt')\n",
    "print(\"YOLO model loaded successfully!\")\n",
    "\n",
    "text_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "text_tokenizer.pad_token = text_tokenizer.eos_token\n",
    "text_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "print(\"GPT-2 model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2312081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving base models...\n",
      "Base models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Save base models\n",
    "print(\"Saving base models...\")\n",
    "os.makedirs('../saved_models', exist_ok=True)\n",
    "os.makedirs('../saved_models/text_generator', exist_ok=True)\n",
    "\n",
    "torch.save(yolo_model.model.state_dict(), '../saved_models/yolo_model_base.pt')\n",
    "text_model.save_pretrained('../saved_models/text_generator/')\n",
    "text_tokenizer.save_pretrained('../saved_models/text_generator/')\n",
    "print(\"Base models saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d82309ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Define pipeline functions\n",
    "def test_yolo_detection(image_path):\n",
    "    \"\"\"Test YOLO object detection\"\"\"\n",
    "    results = yolo_model(image_path)\n",
    "    detected_objects = []\n",
    "    \n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            class_id = int(box.cls[0])\n",
    "            class_name = yolo_model.names[class_id]\n",
    "            confidence = float(box.conf[0])\n",
    "            detected_objects.append({\n",
    "                'class': class_name,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "    \n",
    "    return detected_objects\n",
    "\n",
    "def generate_description(detected_objects):\n",
    "    \"\"\"Generate description based on detected objects\"\"\"\n",
    "    if not detected_objects:\n",
    "        return \"No objects detected in the image.\"\n",
    "    \n",
    "    objects_list = \", \".join([obj['class'] for obj in detected_objects[:5]])\n",
    "    prompt = f\"In this image, I can see {objects_list}. This scene appears to be\"\n",
    "    \n",
    "    inputs = text_tokenizer.encode(prompt, return_tensors='pt', max_length=512, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = text_model.generate(\n",
    "            inputs,\n",
    "            max_length=150,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=text_tokenizer.eos_token_id,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    description = text_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return description\n",
    "\n",
    "def test_pipeline(image_path=None):\n",
    "    \"\"\"Test the complete pipeline\"\"\"\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        print(f\"Processing image: {image_path}\")\n",
    "        \n",
    "        detected_objects = test_yolo_detection(image_path)\n",
    "        \n",
    "        print(\"\\nDetected objects:\")\n",
    "        for obj in detected_objects:\n",
    "            print(f\"  - {obj['class']}: {obj['confidence']:.3f}\")\n",
    "        \n",
    "        description = generate_description(detected_objects)\n",
    "        print(f\"\\nGenerated description: {description}\")\n",
    "        \n",
    "        return detected_objects, description\n",
    "    else:\n",
    "        print(\"No valid image path provided\")\n",
    "        return [], \"\"\n",
    "\n",
    "print(\"Pipeline functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bc13ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing basic pipeline...\n",
      "Pipeline ready! Use test_pipeline('your_image_path') to test.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Test basic pipeline\n",
    "print(\"Testing basic pipeline...\")\n",
    "# Example: test_pipeline('path_to_your_image.jpg')\n",
    "print(\"Pipeline ready! Use test_pipeline('your_image_path') to test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97a62442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Dataset downloaded to: C:\\Users\\RDHQ\\.cache\\kagglehub\\datasets\\junkie75\\yolo-tiger-and-lion-labelled-detection\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: OPTIONAL - Download custom dataset\n",
    "# UNCOMMENT THE FOLLOWING LINES TO DOWNLOAD CUSTOM DATASET\n",
    "\n",
    "import kagglehub\n",
    "print(\"Downloading dataset...\")\n",
    "path = kagglehub.dataset_download(\"junkie75/yolo-tiger-and-lion-labelled-detection\")\n",
    "print(f\"Dataset downloaded to: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09326be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 951 images\n",
      "Dataset prepared: Train: 665, Val: 143, Test: 143\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: OPTIONAL - Prepare dataset for YOLO training\n",
    "# UNCOMMENT THE FOLLOWING LINES TO PREPARE DATASET\n",
    "\n",
    "def prepare_yolo_dataset(dataset_path):\n",
    "    \"\"\"Prepare dataset in YOLO format\"\"\"\n",
    "    \n",
    "    dataset_dir = '../datasets/tiger_lion'\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        os.makedirs(f'{dataset_dir}/images/{split}', exist_ok=True)\n",
    "        os.makedirs(f'{dataset_dir}/labels/{split}', exist_ok=True)\n",
    "    \n",
    "    # Find all image files\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    image_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "                image_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images\")\n",
    "    \n",
    "    # Split data\n",
    "    train_files, temp_files = train_test_split(image_files, test_size=0.3, random_state=42)\n",
    "    val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Copy files\n",
    "    def copy_files(file_list, split_type):\n",
    "        for img_path in file_list:\n",
    "            label_path = img_path.replace('images', 'labels').rsplit('.', 1)[0] + '.txt'\n",
    "            \n",
    "            if os.path.exists(label_path):\n",
    "                img_filename = os.path.basename(img_path)\n",
    "                shutil.copy2(img_path, f'{dataset_dir}/images/{split_type}/{img_filename}')\n",
    "                label_filename = os.path.basename(label_path)\n",
    "                shutil.copy2(label_path, f'{dataset_dir}/labels/{split_type}/{label_filename}')\n",
    "    \n",
    "    copy_files(train_files, 'train')\n",
    "    copy_files(val_files, 'val')\n",
    "    copy_files(test_files, 'test')\n",
    "    \n",
    "    print(f\"Dataset prepared: Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}\")\n",
    "    return dataset_dir\n",
    "\n",
    "# Prepare dataset (uncomment the line below when ready)\n",
    "dataset_dir = prepare_yolo_dataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8175f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset YAML created at: ../datasets/tiger_lion\\dataset.yaml\n",
      "Configuration: {'path': 'c:\\\\Users\\\\RDHQ\\\\Downloads\\\\RADI-Image\\\\datasets\\\\tiger_lion', 'train': 'images/train', 'val': 'images/val', 'test': 'images/test', 'nc': 2, 'names': ['tiger', 'lion']}\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: OPTIONAL - Create dataset.yaml configuration\n",
    "# UNCOMMENT THE FOLLOWING LINES TO CREATE DATASET CONFIG\n",
    "\n",
    "def create_dataset_yaml(dataset_dir):\n",
    "    \"\"\"Create dataset.yaml file for YOLO training\"\"\"\n",
    "    dataset_config = {\n",
    "        'path': os.path.abspath(dataset_dir),\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val', \n",
    "        'test': 'images/test',\n",
    "        'nc': 2,\n",
    "        'names': ['tiger', 'lion']\n",
    "    }\n",
    "    \n",
    "    yaml_path = os.path.join(dataset_dir, 'dataset.yaml')\n",
    "    with open(yaml_path, 'w') as f:\n",
    "        yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"Dataset YAML created at: {yaml_path}\")\n",
    "    print(f\"Configuration: {dataset_config}\")\n",
    "    return yaml_path\n",
    "\n",
    "# Create dataset.yaml (uncomment the line below when ready)\n",
    "yaml_path = create_dataset_yaml(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36bfdb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training configuration...\n",
      "Training configuration ready! Device: cpu\n",
      "Using dataset: ../datasets/tiger_lion/dataset.yaml\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: OPTIONAL - Setup training configuration\n",
    "# UNCOMMENT THE FOLLOWING LINES TO SETUP TRAINING\n",
    "\n",
    "print(\"Setting up training configuration...\")\n",
    "\n",
    "# Check if dataset.yaml exists\n",
    "yaml_path = '../datasets/tiger_lion/dataset.yaml'\n",
    "if not os.path.exists(yaml_path):\n",
    "    print(f\"ERROR: Dataset YAML not found at {yaml_path}\")\n",
    "    print(\"Please run Cells 7, 8, and 9 first to download and prepare the dataset.\")\n",
    "else:\n",
    "    training_args = {\n",
    "        'data': yaml_path,\n",
    "        'epochs': 3,\n",
    "        'imgsz': 640,\n",
    "        'batch': 8,\n",
    "        'patience': 5,\n",
    "        'save': True,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'workers': 2,\n",
    "        'lr0': 0.01,\n",
    "        'project': '../runs/detect',\n",
    "        'name': 'tiger_lion_detection',\n",
    "        'exist_ok': True\n",
    "    }\n",
    "\n",
    "    print(f\"Training configuration ready! Device: {training_args['device']}\")\n",
    "    print(f\"Using dataset: {yaml_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f58f197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for training...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.214 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.200  Python-3.11.9 torch-2.0.1+cpu CPU (Intel Core(TM) i7-7700 3.60GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=../datasets/tiger_lion/dataset.yaml, epochs=3, patience=5, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=2, project=../runs/detect, name=tiger_lion_detection, exist_ok=True, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=..\\runs\\detect\\tiger_lion_detection\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011238 parameters, 3011222 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\RDHQ\\Downloads\\RADI-Image\\datasets\\tiger_lion\\labels\\train... 665 images, 0 backgrounds, 0 corrupt: 100%|██████████| 665/665 [00:02<00:00, 327.01it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\RDHQ\\Downloads\\RADI-Image\\datasets\\tiger_lion\\labels\\train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\RDHQ\\Downloads\\RADI-Image\\datasets\\tiger_lion\\labels\\val... 143 images, 0 backgrounds, 0 corrupt: 100%|██████████| 143/143 [00:00<00:00, 269.52it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\RDHQ\\Downloads\\RADI-Image\\datasets\\tiger_lion\\labels\\val.cache\n",
      "Plotting labels to ..\\runs\\detect\\tiger_lion_detection\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m..\\runs\\detect\\tiger_lion_detection\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/3         0G       1.06      2.453      1.549          3        640: 100%|██████████| 84/84 [06:14<00:00,  4.46s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:36<00:00,  4.05s/it]\n",
      "                   all        143        197      0.425      0.243       0.22     0.0684\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        2/3         0G      1.168      2.049      1.612          2        640: 100%|██████████| 84/84 [06:11<00:00,  4.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:34<00:00,  3.78s/it]\n",
      "                   all        143        197      0.528      0.539      0.497      0.262\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        3/3         0G      1.103      1.879       1.57          1        640: 100%|██████████| 84/84 [06:15<00:00,  4.47s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:33<00:00,  3.76s/it]\n",
      "                   all        143        197      0.556      0.595      0.564       0.28\n",
      "\n",
      "3 epochs completed in 0.341 hours.\n",
      "Optimizer stripped from ..\\runs\\detect\\tiger_lion_detection\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from ..\\runs\\detect\\tiger_lion_detection\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating ..\\runs\\detect\\tiger_lion_detection\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.200  Python-3.11.9 torch-2.0.1+cpu CPU (Intel Core(TM) i7-7700 3.60GHz)\n",
      "Model summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:29<00:00,  3.23s/it]\n",
      "                   all        143        197      0.557      0.595      0.565       0.28\n",
      "                 tiger        143        105      0.644      0.505      0.556       0.29\n",
      "                  lion        143         92       0.47      0.685      0.574       0.27\n",
      "Speed: 2.6ms preprocess, 176.0ms inference, 0.0ms loss, 12.6ms postprocess per image\n",
      "Results saved to \u001b[1m..\\runs\\detect\\tiger_lion_detection\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: OPTIONAL - Start training\n",
    "# UNCOMMENT THE FOLLOWING LINES TO START TRAINING\n",
    "\n",
    "print(\"Loading model for training...\")\n",
    "train_model = YOLO('yolov8n.pt')  # Separate model for training\n",
    "\n",
    "print(\"Starting training...\")\n",
    "results = train_model.train(**training_args)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bde10217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving trained model...\n",
      "Best model already saved at: ../runs/detect/tiger_lion_detection/weights/best.pt\n",
      "Trained model copied to saved_models directory!\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: OPTIONAL - Save trained model (SIMPLIFIED)\n",
    "# UNCOMMENT THE FOLLOWING LINES TO SAVE TRAINED MODEL\n",
    "\n",
    "print(\"Saving trained model...\")\n",
    "\n",
    "# YOLO automatically saves the best model during training, so we don't need to export\n",
    "# Just verify that the training files exist\n",
    "best_model_path = '../runs/detect/tiger_lion_detection/weights/best.pt'\n",
    "last_model_path = '../runs/detect/tiger_lion_detection/weights/last.pt'\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f\"Best model already saved at: {best_model_path}\")\n",
    "    # Copy to our saved_models directory for easier access\n",
    "    shutil.copy(best_model_path, '../saved_models/yolo_model_trained.pt')\n",
    "    print(\"Trained model copied to saved_models directory!\")\n",
    "else:\n",
    "    print(\"No best.pt found. Training might have failed or not completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fc03aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model...\n",
      "Found trained model in saved_models directory\n",
      "Trained model loaded successfully from: ../saved_models/yolo_model_trained.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: OPTIONAL - Load trained model (SIMPLIFIED)\n",
    "# UNCOMMENT THE FOLLOWING LINES TO LOAD TRAINED MODEL\n",
    "\n",
    "print(\"Loading trained model...\")\n",
    "\n",
    "# Check for trained model in multiple locations\n",
    "trained_model_path = None\n",
    "\n",
    "# First check our saved_models directory\n",
    "if os.path.exists('../saved_models/yolo_model_trained.pt'):\n",
    "    trained_model_path = '../saved_models/yolo_model_trained.pt'\n",
    "    print(\"Found trained model in saved_models directory\")\n",
    "\n",
    "# Then check the training runs\n",
    "elif os.path.exists('../runs/detect/tiger_lion_detection/weights/best.pt'):\n",
    "    trained_model_path = '../runs/detect/tiger_lion_detection/weights/best.pt'\n",
    "    print(\"Found trained model in runs directory\")\n",
    "\n",
    "elif os.path.exists('../runs/detect/tiger_lion_detection/weights/last.pt'):\n",
    "    trained_model_path = '../runs/detect/tiger_lion_detection/weights/last.pt'\n",
    "    print(\"Found trained model (last.pt) in runs directory\")\n",
    "\n",
    "# Check for torchscript version (what was actually created)\n",
    "elif os.path.exists('../runs/detect/tiger_lion_detection/weights/best.torchscript'):\n",
    "    print(\"Found torchscript model, but need .pt format for YOLO class\")\n",
    "    # We can't directly load torchscript with YOLO class, so use the original best.pt\n",
    "    if os.path.exists('../runs/detect/tiger_lion_detection/weights/best.pt'):\n",
    "        trained_model_path = '../runs/detect/tiger_lion_detection/weights/best.pt'\n",
    "    else:\n",
    "        print(\"No .pt format found for torchscript model\")\n",
    "\n",
    "if trained_model_path:\n",
    "    yolo_model = YOLO(trained_model_path)\n",
    "    print(f\"Trained model loaded successfully from: {trained_model_path}\")\n",
    "else:\n",
    "    print(\"No trained model found. Using base YOLO model.\")\n",
    "    print(\"Available files in runs directory:\")\n",
    "    runs_dir = '../runs/detect/tiger_lion_detection/weights'\n",
    "    if os.path.exists(runs_dir):\n",
    "        for file in os.listdir(runs_dir):\n",
    "            print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ef505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned column names:\n",
      "  - 'epoch'\n",
      "  - 'train/box_loss'\n",
      "  - 'train/cls_loss'\n",
      "  - 'train/dfl_loss'\n",
      "  - 'metrics/precision(B)'\n",
      "  - 'metrics/recall(B)'\n",
      "  - 'metrics/mAP50(B)'\n",
      "  - 'metrics/mAP50-95(B)'\n",
      "  - 'val/box_loss'\n",
      "  - 'val/cls_loss'\n",
      "  - 'val/dfl_loss'\n",
      "  - 'lr/pg0'\n",
      "  - 'lr/pg1'\n",
      "  - 'lr/pg2'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TRAINING SUMMARY (3 Epochs)\n",
      "==================================================\n",
      "Final Precision: 0.5555\n",
      "Final Recall: 0.5948\n",
      "Final mAP50: 0.5645\n",
      "Final mAP50-95: 0.2801\n",
      "Final Train Box Loss: 1.1025\n",
      "Final Val Box Loss: 1.4007\n",
      "\n",
      "Improvement from Epoch 1 to 3:\n",
      "Precision: +0.1306\n",
      "Recall: +0.3518\n",
      "mAP50: +0.3445\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: OPTIONAL - Visualize training results (FIXED FOR SPACES IN COLUMNS)\n",
    "# UNCOMMENT THE FOLLOWING LINES FOR VISUALIZATION\n",
    "\n",
    "def plot_training_results_fixed():\n",
    "    \"\"\"Plot training metrics - fixed for column names with spaces\"\"\"\n",
    "    try:\n",
    "        results_file = '../runs/detect/tiger_lion_detection/results.csv'\n",
    "        if os.path.exists(results_file):\n",
    "            df = pd.read_csv(results_file)\n",
    "            \n",
    "            # Clean column names by stripping whitespace\n",
    "            df.columns = df.columns.str.strip()\n",
    "            print(\"Cleaned column names:\")\n",
    "            for col in df.columns:\n",
    "                print(f\"  - '{col}'\")\n",
    "            \n",
    "            # Create figure with subplots\n",
    "            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "            fig.suptitle('Training Metrics (3 Epochs)', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            # Plot 1: Precision & Recall\n",
    "            ax1.plot(df['epoch'], df['metrics/precision(B)'], 'b-', label='Precision', linewidth=2, marker='o')\n",
    "            ax1.plot(df['epoch'], df['metrics/recall(B)'], 'r-', label='Recall', linewidth=2, marker='s')\n",
    "            ax1.set_title('Precision & Recall')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Score')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 2: mAP Metrics\n",
    "            ax2.plot(df['epoch'], df['metrics/mAP50(B)'], 'g-', label='mAP50', linewidth=2, marker='o')\n",
    "            ax2.plot(df['epoch'], df['metrics/mAP50-95(B)'], 'orange', label='mAP50-95', linewidth=2, marker='s')\n",
    "            ax2.set_title('mAP Metrics')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('mAP Score')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 3: Box Loss\n",
    "            ax3.plot(df['epoch'], df['train/box_loss'], 'b-', label='Train Box Loss', linewidth=2, marker='o')\n",
    "            ax3.plot(df['epoch'], df['val/box_loss'], 'r-', label='Val Box Loss', linewidth=2, marker='s')\n",
    "            ax3.set_title('Box Loss')\n",
    "            ax3.set_xlabel('Epoch')\n",
    "            ax3.set_ylabel('Loss')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot 4: Classification Loss\n",
    "            ax4.plot(df['epoch'], df['train/cls_loss'], 'b-', label='Train Cls Loss', linewidth=2, marker='o')\n",
    "            ax4.plot(df['epoch'], df['val/cls_loss'], 'r-', label='Val Cls Loss', linewidth=2, marker='s')\n",
    "            ax4.set_title('Classification Loss')\n",
    "            ax4.set_xlabel('Epoch')\n",
    "            ax4.set_ylabel('Loss')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Print training summary\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"TRAINING SUMMARY (3 Epochs)\")\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Final Precision: {df['metrics/precision(B)'].iloc[-1]:.4f}\")\n",
    "            print(f\"Final Recall: {df['metrics/recall(B)'].iloc[-1]:.4f}\")\n",
    "            print(f\"Final mAP50: {df['metrics/mAP50(B)'].iloc[-1]:.4f}\")\n",
    "            print(f\"Final mAP50-95: {df['metrics/mAP50-95(B)'].iloc[-1]:.4f}\")\n",
    "            print(f\"Final Train Box Loss: {df['train/box_loss'].iloc[-1]:.4f}\")\n",
    "            print(f\"Final Val Box Loss: {df['val/box_loss'].iloc[-1]:.4f}\")\n",
    "            \n",
    "            # Show improvement from first to last epoch\n",
    "            if len(df) > 1:\n",
    "                print(\"\\nImprovement from Epoch 1 to 3:\")\n",
    "                precision_improve = df['metrics/precision(B)'].iloc[-1] - df['metrics/precision(B)'].iloc[0]\n",
    "                recall_improve = df['metrics/recall(B)'].iloc[-1] - df['metrics/recall(B)'].iloc[0]\n",
    "                map50_improve = df['metrics/mAP50(B)'].iloc[-1] - df['metrics/mAP50(B)'].iloc[0]\n",
    "                print(f\"Precision: {precision_improve:+.4f}\")\n",
    "                print(f\"Recall: {recall_improve:+.4f}\")\n",
    "                print(f\"mAP50: {map50_improve:+.4f}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"Results file not found at: {results_file}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting training results: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "plot_training_results_fixed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".radi_image_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
