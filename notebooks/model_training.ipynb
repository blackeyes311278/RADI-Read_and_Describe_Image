{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f51a79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv8 model...\n",
      "YOLO model loaded successfully!\n",
      "Loading GPT-2 model for text generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PyConda\\miniconda3\\envs\\rdi_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generation model loaded successfully!\n",
      "Saving models...\n",
      "Models saved successfully!\n",
      "Testing pipeline...\n"
     ]
    }
   ],
   "source": [
    "### Cell : Import libraries\n",
    "import torch\n",
    "import torchvision\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "### Cell : Load and prepare YOLO model (using pre-trained)\n",
    "print(\"Loading YOLOv8 model...\")\n",
    "yolo_model = YOLO('yolov8n.pt')  # Using nano version for faster inference\n",
    "\n",
    "# Test YOLO on sample image\n",
    "print(\"YOLO model loaded successfully!\")\n",
    "\n",
    "### Cell : Load and prepare Text Generation model\n",
    "print(\"Loading GPT-2 model for text generation...\")\n",
    "text_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "text_tokenizer.pad_token = text_tokenizer.eos_token\n",
    "text_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "print(\"Text generation model loaded successfully!\")\n",
    "\n",
    "### Cell 5: Save models in appropriate formats\n",
    "print(\"Saving models...\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('../saved_models', exist_ok=True)\n",
    "os.makedirs('../saved_models/text_generator', exist_ok=True)\n",
    "\n",
    "# Save YOLO model (PyTorch format)\n",
    "torch.save(yolo_model.model.state_dict(), '../saved_models/yolo_model.pt')\n",
    "\n",
    "# Save text generation model and tokenizer\n",
    "text_model.save_pretrained('../saved_models/text_generator/')\n",
    "text_tokenizer.save_pretrained('../saved_models/text_generator/')\n",
    "\n",
    "print(\"Models saved successfully!\")\n",
    "\n",
    "### Cell : Test the models\n",
    "def test_yolo_detection(image_path):\n",
    "    \"\"\"Test YOLO object detection\"\"\"\n",
    "    results = yolo_model(image_path)\n",
    "    detected_objects = []\n",
    "    \n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            class_id = int(box.cls[0])\n",
    "            class_name = yolo_model.names[class_id]\n",
    "            confidence = float(box.conf[0])\n",
    "            detected_objects.append({\n",
    "                'class': class_name,\n",
    "                'confidence': confidence\n",
    "            })\n",
    "    \n",
    "    return detected_objects\n",
    "\n",
    "def generate_description(detected_objects):\n",
    "    \"\"\"Generate description based on detected objects\"\"\"\n",
    "    objects_list = \", \".join([obj['class'] for obj in detected_objects[:5]])\n",
    "    prompt = f\"In this image, I can see {objects_list}. This scene appears to be\"\n",
    "    \n",
    "    inputs = text_tokenizer.encode(prompt, return_tensors='pt', max_length=512, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = text_model.generate(\n",
    "            inputs,\n",
    "            max_length=150,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=text_tokenizer.eos_token_id,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    description = text_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return description\n",
    "\n",
    "# Test with a sample image (you can upload any image)\n",
    "print(\"Testing pipeline...\")\n",
    "# For testing, you can use any image from COCO dataset or your own"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
